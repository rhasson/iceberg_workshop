{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with branches and Write-Audit-Publish\n",
    "\n",
    "Branches in Iceberg allow you to make changes to your data or table configuration without affecting other users working with the master table branch.\n",
    "WAP is a pattern that works well with branches that allows you to make changes, audit those changes and only when validated, merge them for users to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start up your Spark session configured to use our Polaris Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update with your principal user credentials (from Polaris Catalog)\n",
    "\n",
    "clientId=\"90c6a4746a5434b4\"\n",
    "clientSecret=\"a612b67eec79bd69b76bf9804b66bc96\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start the Spark application and connect to our Polaris Catalog\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('iceberg_lab') \\\n",
    ".config('spark.sql.defaultCatalog', 'polaris') \\\n",
    ".config('spark.sql.catalog.polaris', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.catalog.polaris.type', 'rest') \\\n",
    ".config('spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation','true') \\\n",
    ".config('spark.sql.catalog.polaris.client.region','us-east-1') \\\n",
    ".config('spark.sql.catalog.polaris.uri','http://polaris-catalog:8181/api/catalog') \\\n",
    ".config('spark.sql.catalog.polaris.credential',clientId+':'+clientSecret) \\\n",
    ".config('spark.sql.catalog.polaris.warehouse','polariscatalog') \\\n",
    ".config('spark.sql.catalog.polaris.scope','PRINCIPAL_ROLE:ALL') \\\n",
    ".config('spark.sql.catalog.polaris.token-refresh-enabled', 'true') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some data\n",
    "\n",
    "We'll use the famous NYC taxi data for this excercise.  It's available on their website, but we took a recent snapshot and made it available in the Upsolver S3 bucket.\n",
    "\n",
    "We'll use Spark DataFrames to read Taxi data CSV into an Iceberg table.\n",
    "\n",
    "NOTE: if you're starting with this notebook and didn't complete the Getting Started with Spark and Iceberg notebook, than you need to first create a `demo` database in the `polaris` catalog, like `CREATE DATABASE polaris.demo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### More information can be found on the NYC Public Data website\n",
    "### https://data.cityofnewyork.us/Transportation/For-Hire-Vehicles-FHV-Active/8wbx-tsch/about_data\n",
    "\n",
    "### Recent file was downloaded and available for this workshop in the following bucket\n",
    "### s3://upsolver-workshop-lake/workshop-samples/For_Hire_Vehicles_FHV_Active_20240915.csv\n",
    "\n",
    "spark.sql('DROP TABLE IF EXISTS demo.vfh PURGE')\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "          .option(\"header\",True) \\\n",
    "          .option(\"inferschema\",True) \\\n",
    "          .load(\"s3a://upsolver-workshop-lake/workshop-samples/For_Hire_Vehicles_FHV_Active_20240915.csv\")\n",
    "\n",
    "df.write.saveAsTable('demo.vfh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that data has been loaded successfully to our Iceberg table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM demo.vfh limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple aggregation, by taxi company name, so that we can demonstrate working with branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT `Base Name`, `Base Number`, count(*) AS total_cars FROM demo.vfh \n",
    "GROUP BY 1,2\n",
    "ORDER BY 3 DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a branch\n",
    "\n",
    "You can learn more about branches in the Iceberg [documentations](https://iceberg.apache.org/docs/latest/spark-ddl/#branching-and-tagging-ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE demo.vfh\n",
    "CREATE OR REPLACE BRANCH `audit`\n",
    "RETAIN 2 DAYS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with data in a branch\n",
    "\n",
    "To work within your branch you need to refernce it by name with the prefix `branch_` followed by the branch name. The branch is referenced following the table name, like `dbName.tableName.branch_branchName`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll do is delete some data.  We're doing this within a branch so the main table other users are querying will not be affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM demo.vfh.branch_audit\n",
    "WHERE `Base Number` = 'B03557'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the aggregation query on the new branch and validate that indeed SPACELINKS LLC company is no longer in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT `Base Name`, `Base Number`, count(*) AS total_cars \n",
    "FROM demo.vfh.branch_audit\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rerun the same query but on the main branch, just like other users would and ensure the original data is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT `Base Name`, `Base Number`, count(*) AS total_cars \n",
    "FROM demo.vfh\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the `refs` information table to see the snapshot associated with your branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM polaris.demo.vfh.refs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the same view of the data by using the `VERSION AS OF SNAPSHOT_ID`.  You get the snapshot ID from the `refs` or `snapshots` information tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT `Base Name`, `Base Number`, count(*) AS total_cars \n",
    "FROM demo.vfh VERSION AS OF 6215557498076574768  --change this to your snapshot id\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to merge branches, read more in the [documentation](https://iceberg.apache.org/docs/latest/spark-procedures/#snapshot-management).\n",
    "\n",
    "In this example we'll use `cherry_pick` to select our audit branch and promote it to `main`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%sql\n",
    "\n",
    "CALL polaris.system.cherrypick_snapshot('demo.vfh', 6215557498076574768) -- substitute your snapshot ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the merge is complete, we can let the branch automatically expire or we can manually drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE demo.vfh\n",
    "DROP BRANCH audit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with WAP\n",
    "\n",
    "WAP is a pattern that allows you to write some changes, audit them and then publish them to the rest of your users.\n",
    "\n",
    "WAP similar to branching but less explicit.  With branches you create, update, drop or publish data in your branch. With WAP, you enable it on your table and define a session WAP_ID property that Spark implicitly uses to isolate changes to an internal \"branch\".  Other users are unaware unless they dive deep into the snapshot detail.  Once you're happy with your changes, you can publish them to the main branch without the need to manage branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To setup WAP you need to first need to enable WAP on your table and then create a unique ID, we use the `uuid` library for this.  \n",
    "\n",
    "Note that WAP is enabled on the table, hence why we set the `TBLPROPERTIES`.  But the WAP ID is defined as a Spark session property that will only persist within the active Spark session.  If the session is closed, lost or restarted, you'll need to regenerate and re-set the WAP ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "wap_id = uuid.uuid4().hex\n",
    "\n",
    "spark.sql(\"ALTER TABLE demo.vfh SET TBLPROPERTIES ('write.wap.enabled'='true')\")\n",
    "spark.conf.set('spark.wap.id', wap_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets delete some data.  Note we're not referencing the branch name in this case. Spark uses the WAP ID behind the scenes to track the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "delete from demo.vfh\n",
    "where `Base Number` = 'B03406'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since WAP works under the covers, querying the base table will show no changes, all data will be present. To view the changed dataset, you must query it using the `VERSION AS OF SNAPSHOT_ID` syntax shown below.  To get the latest snapshot ID that includes your changes view the `snapshots` information table.\n",
    "\n",
    "When you inspect the `snapshots` information table, notice the `wap-id` field is included in the `summary` column which should have your WAP ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT `Base Name`, `Base Number`, count(*) AS total_cars \n",
    "FROM demo.vfh VERSION AS OF 2263443007379069011 -- substitute for your snapshot id\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 3 desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can validate your data and ensure it is as you expect. \n",
    "\n",
    "If you're happy, publish your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CALL polaris.system.publish_changes('demo.vfh', '\"+ wap_id+\"')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not happy with your changes, simply expire the snapshot that includes your changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## substitute your snapshot id\n",
    "\n",
    "spark.sql(\"CALL polaris.system.expire_snapshots(table => 'demo.vfh', snapshot_ids => Array(2263443007379069011))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you're done, disable WAP on the table and unset the session property containing your WAP ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"ALTER TABLE demo.vfh SET TBLPROPERTIES ('write.wap.enabled'='false')\")\n",
    "spark.conf.unset('spark.wap.id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
