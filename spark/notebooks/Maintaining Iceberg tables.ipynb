{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Iceberg tables\n",
    "\n",
    "In this part of the workshop we'll look at the different ways Iceberg enables you to optimize and maintain your tables.\n",
    "\n",
    "You can learn more in the Iceberg [documentation](https://iceberg.apache.org/docs/latest/spark-procedures/#metadata-management)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Spark\n",
    "\n",
    "Start Spark and connect to your Polaris Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update with your principal user credentials (from Polaris Catalog)\n",
    "\n",
    "clientId=\"0b8097fb53c92862\"\n",
    "clientSecret=\"85c2af291ebdc578d95efd768aeac0e5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start the Spark application and connect to our Polaris Catalog\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('iceberg_lab') \\\n",
    ".config('spark.sql.defaultCatalog', 'polaris') \\\n",
    ".config('spark.sql.catalog.polaris', 'org.apache.iceberg.spark.SparkCatalog') \\\n",
    ".config('spark.sql.catalog.polaris.type', 'rest') \\\n",
    ".config('spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation','true') \\\n",
    ".config('spark.sql.catalog.polaris.client.region','us-east-1') \\\n",
    ".config('spark.sql.catalog.polaris.uri','http://polaris-catalog:8181/api/catalog') \\\n",
    ".config('spark.sql.catalog.polaris.credential',clientId+':'+clientSecret) \\\n",
    ".config('spark.sql.catalog.polaris.warehouse','polariscatalog') \\\n",
    ".config('spark.sql.catalog.polaris.scope','PRINCIPAL_ROLE:ALL') \\\n",
    ".config('spark.sql.catalog.polaris.token-refresh-enabled', 'true') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table and load some data\n",
    "\n",
    "You'll create a table and load some data.  We'll then optimize these files by compacting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "### https://data.cityofnewyork.us/NYC-BigApps/Citi-Bike-System-Data/vsnr-94wk\n",
    "\n",
    "r = requests.get('https://gbfs.citibikenyc.com/gbfs/en/station_status.json')\n",
    "station_status = r.json()\n",
    "\n",
    "with open(\"/home/iceberg/notebooks/station_status.json\", \"w\") as f:\n",
    "    for item in station_status['data']['stations']:\n",
    "        json.dump(item, f)\n",
    "        f.write('\\n\\r')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS demo.stations PURGE')\n",
    "\n",
    "df = spark.read.format(\"json\") \\\n",
    "          .option(\"header\",True) \\\n",
    "          .option(\"inferschema\",True) \\\n",
    "          .load(\"/home/iceberg/notebooks/station_status.json\")\n",
    "\n",
    "df.repartition(100).write.saveAsTable('demo.stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM demo.stations limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many files were created.  In this example, we forced Spark to split the data into 100 files, but in the real world this will happen naturally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(*) FROM polaris.demo.stations.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite data file, aka. compaction\n",
    "\n",
    "Compaction is an important process that combines smalls files into few larger files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by compacting our table by looking for 2 or more files with the smallest size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = spark.sql(\"CALL polaris.system.rewrite_data_files(table => 'demo.stations', options => map('min-input-files','2', 'rewrite-job-order','bytes-asc'))\")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the `files` information table again and you'll see that we only have 1 single file now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(*) FROM polaris.demo.stations.files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Before starting this step, drop the table and recreate it as before so we can test out other compaction scenarios. ***\n",
    "\n",
    "In the following compaction scenario we're sorting the data during compaction. There are bin-packing and sorting using standard ordering or zorder.\n",
    "- Binpacking simply arranges bits to fit more into fewer files.\n",
    "- Sorting organizes rows by sort key so similar data is colocated in the same files making reads more efficient.\n",
    "- Zorder is more complex ordering that comes with its own pros/cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = spark.sql(\"CALL polaris.system.rewrite_data_files(table => 'demo.stations', strategy => 'sort', sort_order => 'station_id DESC NULLS LAST,legacy_id DESC NULLS LAST')\")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting optimization is to compact only those files that meet a specific filter criteria.  This is helpful when there is large skew in the data and the low cardinality data is not often compacted because it's under the file number of byte size threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = spark.sql(\"CALL polaris.system.rewrite_data_files(table => 'demo.stations', where => is_installed = 1)\")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expiring snapshots\n",
    "\n",
    "As you already noticed, Iceberg creates lots of snapshots to keep track of changes.  Each snapshot creates numerous manifest files that track everything about files and partitions and schemas.  Each snapshot is also maintains the full table history so you can time travel in queries. However, all of this takes up storage and cost you money.  \n",
    "\n",
    "It's a good practice to expire old snapshots after some period of time or number of snapshots created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First inspect your `snapshots` information table and lets see which one to expire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM polaris.demo.stations.snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = spark.sql(\"CALL polaris.system.expire_snapshots(table => 'demo.stations', snapshot_ids => ARRAY(642880844932688596))\")\n",
    "ret.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the `snapshots` table again and you'll see the old snapshot was removed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
